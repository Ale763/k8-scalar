\subsection{Context}
Cloud computing has become increasingly popular since its introduction and is expected to keep growing in the years to come \citep{Statista.com}. Companies tend to move their resources to the cloud rather than keeping them on their own infrastructure as it offers numerous advantages (i.e. \citep{AhmadiM.2018Caao}). Cloud computing solutions come in various models: infrastructure as a service (IaaS), platform as a service (PaaS) and software as a service (SaaS). SaaS providers themselves often move their resources to an IaaS cloud. For these providers, cost-efficiency becomes a challenge. \\

SaaS providers aim to provide their services as cost-efficiently as possible. In this regard, they face the continuous challenge of utilizing only a minimal amount of resources while still meeting their customers' requirements, in order to reduce the cost. One way to reduce the amount of resources needed is by locating multiple applications on the same node. This entails new challenges, such as determining how to divide overprovisioned resources among the deployed applications, and how to handle resource contention in general.  \\

Kubernetes is a popular open source framework for managing containerized applications in a distributed environment, providing basic mechanisms for deployment, maintenance and scaling of applications\citep{kubernetes_github}. It provides a container-centric management environment\citep{what_is_kubernetes}, enabling portability across infrastructure providers. It allows users to write their own controllers, such as schedulers, with their own APIs that can be targeted by a general-purpose command-line tool. \\


Kubernetes offers several useful features for resource management. A pod in Kubernetes is the smallest deployable unit of computing which can be created and managed\citep{pod}. Containers belonging to the same application are grouped together in pods. The resources used by a pod can be limited, for example, by setting its so-called requests and limits. The request of a pod is the amount of resources which it is guaranteed to get, the limit is the maximal amount of resources it can obtain\citep{requestlimit}. Requests and limits can be set for both CPU and memory. Kubernetes also offers the ability to set the priority of a pod. If there are insufficient resources available in a node, the pods with lower priorities will be throttled or evicted. This frees up resources in the node for the other pods.\\

%\newpage

Another way of adjusting the resources available to an application is to scale it. The two main methods of doing so are horizontal and vertical scaling. With horizontal scaling, an application is granted more resources by deploying new replicas of that application. With vertical scaling, the existing replicas are provided additional resources. Scaling can be done either manually or by implementing an autoscaler, a piece of software which automatically monitors applications and scales them when needed. Kubernetes offers default horizontal and vertical autoscalers, called the Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) respectively.

\subsection{Problem statement}
So far, research on resource management in container-based environments has been rather scarce, as it is still relatively new compared to VM-based environments \citep{Al-DhuraibiYahya2018EiCC}\citep{CoutinhoEmanuel2015Eicc}. Kubernetes is evolving rapidly. Meanwhile, both documentation and research are struggling to keep up with its development, especially since most research is aimed towards custom scaling techniques (i.e. \citep{hyscale}\citep{caravel}). As a result, identifying the optimal way to manage resources in Kubernetes can be a difficult task. Multiple questions may arise when configuring a Kubernetes cluster, such as how to choose suitable requests and limits, on which nodes to locate certain pods and how to configure an autoscaler. The goal of this thesis is therefore to research how different Kubernetes features can be leveraged to effectively manage resources in the framework. 

\subsection{Approach}
This thesis describes the effects of using vertical scaling through the use of requests and limits, horizontal scaling using the default Kubernetes HPA, and a combination of both, in different environments. A test application deployed on a Kubernetes cluster is subjected to multiple experiments to illustrate the impact of different configurations:

\begin{itemize}
    \item \textbf{Requests and limits}
    The impact of different request and limit configurations is examined by deploying another, low priority pod on the same node as the main application's pod. This impact is described by providing answers to two questions: first, how are resources divided among these pods; second, what is the impact of the co-location on the performance of the higher priority pod? The first question is answered by subjecting the pods to different configurations and observing their resource usage when a workload is applied. The second question is answered by monitoring the performance of the higher priority application. If the high priority pod's performance is not limited by co-locating it with a lower priority pod, then said co-location would increase resource utilization without any downside.
    \item \textbf{The Kubernetes HPA}
    If the user load rises to a point where SLAs get violated even if lower priority pods get throttled or evicted, applications may need to be scaled to provide the desired services. The performance impact of adding the default Kubernetes horizontal autoscaler to a cluster is illustrated. Furthermore, the effects of combining the autoscaler with the request and limit mechanisms, are described.
\end{itemize}

Different configurations may result in different performance gains (or losses) depending on the environment. For example, the user load may be very bursty, in which case using the oversubscription concepts or the HPA may not always lead to the expected results. Hence, the aforementioned impacts of the different mechanisms will be tested for both linearly increasing and bursty workloads. In this thesis, however, only CPU intensive workloads will be examined. Scaling memory bound systems is less common as performing a static optimization of needed memory often suffices. Furthermore, Kubernetes does not offer support for configuring network and disk resources.

\subsection{Research questions}
This thesis aims to provide insight into how different Kubernetes features can be employed to meet SLA requirements cost-efficiently. The Kubernetes features show potential to be effective tools for adaptive resource management and server consolidation purposes. Specifically, this thesis intends to provide clear answers to the following questions:

\begin{itemize}
    \item What is the impact of different request and limit configurations?
    \item Does co-locating a high priority pod with a low priority pod affect the performance of the high priority pod?
    \item What is the performance impact of scaling an application using the HPA?
    \item Does the type of user load change the answer to any of the previous questions?
\end{itemize}



\subsection{Overview of text}
The thesis is structured as follows. Chapter \ref{chap:background} contains a brief overview of all the used concepts, the most important ones being Kubernetes and its tools for resource management. Chapter \ref{chap:existing_research} discusses relevant related work on resource management in cloud environments. Chapter \ref{chap:test_env} provides an overview of the used test environment, focusing on the cluster setup and the used test applications. In section \ref{chap:results}, the experiments are described and their results are show. Finally, section \ref{chap:conclusion} presents a conclusion and possible starting points for future work.
